{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32f3118c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntorch.max 函数的demo\\nnum = 10\\na = torch.arange(num).reshape(num,1)\\nb = 5*torch.ones(num,1)\\nc = torch.cat((a-b, torch.zeros(num,1)), 1)\\nd = torch.max(c, 1)\\nprint(d.values.reshape(10, 1))\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "'''\n",
    "torch.max 函数的demo\n",
    "num = 10\n",
    "a = torch.arange(num).reshape(num,1)\n",
    "b = 5*torch.ones(num,1)\n",
    "c = torch.cat((a-b, torch.zeros(num,1)), 1)\n",
    "d = torch.max(c, 1)\n",
    "print(d.values.reshape(10, 1))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ccff2f",
   "metadata": {},
   "source": [
    "需要求解的问题是：\n",
    "$$\\frac{\\partial u}{\\partial t}+\\frac{1}{2} \\sigma^2 S^2 \\frac{\\partial^2 u}{\\partial S^2}+r S \\frac{\\partial u}{\\partial S}-r u=0$$\n",
    "$$u(0, t)=0 , u\\left(S_{\\max }, t\\right)=S_{\\max }-K,$$\n",
    "$$\n",
    "u(S, T)=\\max (S-K, 0),\n",
    "$$\n",
    "其中$S$是原生资产价格，$t$是时间，$r$是无风险利率，$\\sigma$是波动率.\n",
    "$$sigma = 0.3;\\quad          \\% volatility$$\n",
    "$$r = 0.25;\\quad              \\% interest rate $$\n",
    "$$K = 10; \\quad               \\% strike price$$\n",
    "$$S_{\\max } = 50; \\quad               \\% asset value$$\n",
    "$$T = 1; \\quad                \\% time$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb541ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain and Sampling\n",
    "def interior(n=1000):\n",
    "    S = 50*torch.rand(n, 1)\n",
    "    t = torch.rand(n, 1)\n",
    "    # 0\n",
    "    cond = torch.zeros_like(S)\n",
    "    return S.requires_grad_(True), t.requires_grad_(True), cond\n",
    "\n",
    "\n",
    "def up(n=100):\n",
    "    S = 50*torch.rand(n, 1)\n",
    "    t = torch.ones_like(S)\n",
    "    # max(S-K,0)\n",
    "    cond = torch.max(torch.cat(((S-10*torch.ones(n,1)), torch.zeros(n,1)),1), 1)\n",
    "    return S.requires_grad_(True), t.requires_grad_(True), cond\n",
    "\n",
    "\n",
    "def left(n=100):\n",
    "    t = torch.rand(n, 1)\n",
    "    S = torch.zeros_like(t)\n",
    "    # 0\n",
    "    cond = torch.zeros_like(S)\n",
    "    return S.requires_grad_(True), t.requires_grad_(True), cond\n",
    "\n",
    "\n",
    "def right(n=100):\n",
    "    t = torch.rand(n, 1)\n",
    "    S = 50*torch.ones_like(t)\n",
    "    # S_max-K\n",
    "    cond = S - 10*torch.ones_like(S)\n",
    "    return S.requires_grad_(True), t.requires_grad_(True), cond"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee7596",
   "metadata": {},
   "source": [
    "定义$2\\times 32\\times 32\\times 32\\times 1$的神经网络，激活函数用Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "04e33623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 32),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678a3b35",
   "metadata": {},
   "source": [
    "损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "263dfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def gradients(u, x, order=1):\n",
    "    if order == 1:\n",
    "        return torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u),\n",
    "                                   create_graph=True,\n",
    "                                   only_inputs=True, )[0]\n",
    "    else:\n",
    "        return gradients(gradients(u, x), x, order=order - 1)\n",
    "\n",
    "\n",
    "    # x -> S, y -> t\n",
    "def l_interior(u):\n",
    "    x, y, cond = interior()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(gradients(uxy, y, 1) \\\n",
    "                + 0.5*0.09*x**2*gradients(uxy, x, 2) \\\n",
    "                + 0.25*x*gradients(uxy, x, 1) \\\n",
    "                - 0.25*uxy\\\n",
    "                , cond)\n",
    "\n",
    "\n",
    "def l_up(u):\n",
    "    x, y, cond = up()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "\n",
    "def l_left(u):\n",
    "    x, y, cond = left()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n",
    "\n",
    "\n",
    "def l_right(u):\n",
    "    x, y, cond = right()\n",
    "    uxy = u(torch.cat([x, y], dim=1))\n",
    "    return loss(uxy, cond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2e6c540",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'torch.return_types.max' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m):\n\u001b[0;32m      5\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m      6\u001b[0m     l \u001b[38;5;241m=\u001b[39m l_interior(u) \\\n\u001b[1;32m----> 7\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[43ml_up\u001b[49m\u001b[43m(\u001b[49m\u001b[43mu\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;241m+\u001b[39m l_left(u) \\\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;241m+\u001b[39m l_right(u)\n\u001b[0;32m     10\u001b[0m     l\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     11\u001b[0m     opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m, in \u001b[0;36ml_up\u001b[1;34m(u)\u001b[0m\n\u001b[0;32m     26\u001b[0m x, y, cond \u001b[38;5;241m=\u001b[39m up()\n\u001b[0;32m     27\u001b[0m uxy \u001b[38;5;241m=\u001b[39m u(torch\u001b[38;5;241m.\u001b[39mcat([x, y], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloss\u001b[49m\u001b[43m(\u001b[49m\u001b[43muxy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\zhengtjlab-jczhao\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\zhengtjlab-jczhao\\miniconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\zhengtjlab-jczhao\\miniconda3\\lib\\site-packages\\torch\\nn\\functional.py:3281\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, target):\n\u001b[0;32m   3278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m   3279\u001b[0m         mse_loss, (\u001b[38;5;28minput\u001b[39m, target), \u001b[38;5;28minput\u001b[39m, target, size_average\u001b[38;5;241m=\u001b[39msize_average, reduce\u001b[38;5;241m=\u001b[39mreduce, reduction\u001b[38;5;241m=\u001b[39mreduction\n\u001b[0;32m   3280\u001b[0m     )\n\u001b[1;32m-> 3281\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m   3282\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   3283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis will likely lead to incorrect results due to broadcasting. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(target\u001b[38;5;241m.\u001b[39msize(), \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()),\n\u001b[0;32m   3286\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m   3287\u001b[0m     )\n\u001b[0;32m   3288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'torch.return_types.max' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "u = MLP()\n",
    "opt = torch.optim.Adam(params=u.parameters())\n",
    "for i in range(10000):\n",
    "    opt.zero_grad()\n",
    "    l = l_interior(u) \\\n",
    "        + l_up(u) \\\n",
    "        + l_left(u) \\\n",
    "        + l_right(u)\n",
    "    l.backward()\n",
    "    opt.step()\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(\"{:.2%}\".format((i + 1)/10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51d9d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "xc = torch.linspace(0, 1, 100)\n",
    "xx, yy = torch.meshgrid(50*xc, xc)\n",
    "xx = xx.reshape(-1, 1)\n",
    "yy = yy.reshape(-1, 1)\n",
    "xy = torch.cat([xx, yy], dim=1)\n",
    "u_pred = u(xy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
